{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옵션값을 입력하세요 0:SGD, 1:adagrad, 2:SGD+Dropout, 3:adagrad+Dropout\n",
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"옵션값을 입력하세요 0:SGD, 1:adagrad, 2:SGD+Dropout, 3:adagrad+Dropout\")\n",
    "overfitting=input()\n",
    "\n",
    "if overfitting=='0':\n",
    "    iris_data = datasets.load_iris()\n",
    "    input_data = iris_data.data\n",
    "    correct = iris_data.target\n",
    "    n_data = len(correct)  # 샘플 수\n",
    "\n",
    "# -- 입력 데이터 표준화 --\n",
    "    ave_input = np.average(input_data, axis=0)\n",
    "    std_input = np.std(input_data, axis=0)\n",
    "    input_data = (input_data - ave_input) / std_input\n",
    "\n",
    "# -- 정답을 원-핫 인코딩으로 변경 --\n",
    "    correct_data = np.zeros((n_data, 3))\n",
    "    for i in range(n_data):\n",
    "        correct_data[i, correct[i]] = 1.0\n",
    "\n",
    "# -- 훈련 데이터와 테스트 데이터 --\n",
    "    index = np.arange(n_data)\n",
    "    index_train = index[index%2 == 0]\n",
    "    index_test = index[index%2 != 0]\n",
    "\n",
    "    input_train = input_data[index_train, :]  # 훈련데이터 입력\n",
    "    correct_train = correct_data[index_train, :]  # 훈련데이터 정답\n",
    "    input_test = input_data[index_test, :]  # 테스트데이터 입력\n",
    "    correct_test = correct_data[index_test, :]  # 테스트데이터 정답\n",
    "\n",
    "    n_train = input_train.shape[0]  # 훈련데이터 샘플 수\n",
    "    n_test = input_test.shape[0]  # 테스트데이터 샘플 수\n",
    "\n",
    "# -- 각 설정 값 --\n",
    "    n_in = 4  # 입력층 뉴런 수\n",
    "    n_mid = 25  # 은닉층 뉴런 수\n",
    "    n_out = 3  # 출력층 뉴런 수\n",
    "\n",
    "    wb_width = 0.1  # 가중치와 편향 설정을 위한 정규분포 표준편차\n",
    "    eta = 0.01  # 학습률\n",
    "    epoch = 1000\n",
    "    batch_size = 8\n",
    "    interval = 100  # 경과 표시 간격\n",
    "    n_batch = n_train // batch_size  # 1에포크 당 배치 수\n",
    "   \n",
    "    #2) 클래스 구현\n",
    "    class BaseLayer:\n",
    "        def __init__(self,n_upper,n): #상위층에 뉴런의 갯수, 현재 층의 뉴런의 갯수\n",
    "            self.w=wb_width*np.random.randn(n_upper,n) # 가중치와 편향 랜덤 주기\n",
    "            self.b=wb_width*np.random.randn(n)\n",
    "        \n",
    "        def update(self,eta):\n",
    "            self.w -=eta*self.grad_w # 가중치와 편향의 기울기 * 학습률\n",
    "            self.b -=eta*self.grad_b\n",
    "\n",
    "    class MiddleLayer(BaseLayer): #상속 은닉층\n",
    "        #순방향 \n",
    "        def forward(self,x):\n",
    "            self.x=x\n",
    "            self.u=np.dot(x,self.w) + self.b # 행렬 곱 가중치와 입력들어온 x값\n",
    "            self.y=np.where(self.u<=0,0,self.u) # (활성화함수)출력 하는거임 ReLu 함수 0보다 작으면 0, 아니면 그 자신\n",
    "        \n",
    "        #역방향\n",
    "        def backward(self,grad_y):\n",
    "            delta=grad_y*np.where(self.u<=0,0,1) # 출력기울기*relu 함수 미분\n",
    "            self.grad_w=np.dot(self.x.T,delta)#가중치와 편향와 입력의 기울기를 구해야함.\n",
    "            self.grad_b=np.sum(delta,axis=0)\n",
    "            self.grad_x=np.dot(delta,self.w.T) #T는 전치행렬\n",
    "\n",
    "    class OutputLayer(BaseLayer): #상속 출력층\n",
    "        #순방향 \n",
    "        def forward(self,x):\n",
    "            self.x=x\n",
    "            self.u=np.dot(x,self.w) + self.b\n",
    "            self.y=np.exp(self.u)/np.sum(np.exp(self.u),axis=1,keepdims=True)\n",
    "        \n",
    "        #역방향\n",
    "        def backward(self,t): # t는 정답 정답을 입력으로 받음\n",
    "            delta=self.y-t\n",
    "            self.grad_w=np.dot(self.x.T,delta)#가중치와 편향와 입력의 기울기를 구해야함.\n",
    "            self.grad_b=np.sum(delta,axis=0)\n",
    "            self.grad_x=np.dot(delta,self.w.T) #T는 전치행렬\n",
    "        \n",
    "    #3)신경망 구현\n",
    "    #학습 알고리즘 - 미니배치 사용 \n",
    "    \n",
    "    middle_layer_1=MiddleLayer(n_in,n_mid)\n",
    "    middle_layer_2=MiddleLayer(n_mid,n_mid)\n",
    "    output_layer=OutputLayer(n_mid,n_out)\n",
    "\n",
    "    #순전파\n",
    "    def forward_propagation(x):\n",
    "        middle_layer_1.forward(x)\n",
    "        middle_layer_2.forward(middle_layer_1.y) #앞 뉴런의 출력\n",
    "        output_layer.forward(middle_layer_2.y) #위와 동일\n",
    "    \n",
    "    #역전파\n",
    "    def back_propagation(t):\n",
    "        output_layer.backward(t)\n",
    "        middle_layer_2.backward(output_layer.grad_x) #입력 기울기\n",
    "        middle_layer_1.backward(middle_layer_2.grad_x)\n",
    "    \n",
    "    #보정\n",
    "    def update_wb():\n",
    "        middle_layer_1.update(eta)\n",
    "        middle_layer_2.update(eta)\n",
    "        output_layer.update(eta)\n",
    "    \n",
    "    #교차엔트로피 에러 계산하는 함수\n",
    "    def get_error(t,batch_size): #8개를 다 구한다음 배치사이즈 만큼 나눠줌\n",
    "        return -np.sum(t*np.log(output_layer.y+1e-7))\n",
    "    \n",
    "    \n",
    "    # 학습 for문 두개로 / 후 출력\n",
    "\n",
    "    train_error_x=[] # 데이터의 횟수(갯수)\n",
    "    train_error_y=[] # 데이터의 오차값\n",
    "    test_error_x=[]\n",
    "    test_error_y=[]\n",
    "\n",
    "    for i in range(epoch):\n",
    "        forward_propagation(input_train)\n",
    "        error_train=get_error(correct_train,n_train) # 한번에 다 보겠다. 그래서 n_train 75개\n",
    "        forward_propagation(input_test)\n",
    "        error_test=get_error(correct_test,n_test)\n",
    "    \n",
    "        train_error_x.append(i)\n",
    "        train_error_y.append(error_train)\n",
    "        test_error_x.append(i)\n",
    "        test_error_y.append(error_test)\n",
    "    \n",
    "        index_random=np.arange(n_train)\n",
    "        np.random.shuffle(index_random)\n",
    "        for j in range(n_batch):\n",
    "            mb_index=index_random[j*batch_size:(j+1)*batch_size] # 0 부터 8 까지 0~7 j 가 1이면 9~15\n",
    "            x= input_train[mb_index,:]\n",
    "            t= correct_train[mb_index,:] #목표출력(정답)\n",
    "        \n",
    "            forward_propagation(x)\n",
    "            back_propagation(t)\n",
    "            update_wb()\n",
    "\n",
    "    plt.plot(train_error_x,train_error_y,label=\"Train\")\n",
    "    plt.plot(test_error_x,test_error_y,label=\"Test\")\n",
    "    plt.legend() # 각 그래프 이름\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.show()\n",
    "\n",
    "if overfitting=='1':\n",
    "    iris_data = datasets.load_iris()\n",
    "    input_data = iris_data.data\n",
    "    correct = iris_data.target\n",
    "    n_data = len(correct)  # 샘플 수\n",
    "\n",
    "# -- 입력 데이터 표준화 --\n",
    "    ave_input = np.average(input_data, axis=0)\n",
    "    std_input = np.std(input_data, axis=0)\n",
    "    input_data = (input_data - ave_input) / std_input\n",
    "\n",
    "# -- 정답을 원-핫 인코딩으로 변경 --\n",
    "    correct_data = np.zeros((n_data, 3))\n",
    "    for i in range(n_data):\n",
    "        correct_data[i, correct[i]] = 1.0\n",
    "\n",
    "# -- 훈련 데이터와 테스트 데이터 --\n",
    "    index = np.arange(n_data)\n",
    "    index_train = index[index%2 == 0]\n",
    "    index_test = index[index%2 != 0]\n",
    "\n",
    "    input_train = input_data[index_train, :]  # 훈련데이터 입력\n",
    "    correct_train = correct_data[index_train, :]  # 훈련데이터 정답\n",
    "    input_test = input_data[index_test, :]  # 테스트데이터 입력\n",
    "    correct_test = correct_data[index_test, :]  # 테스트데이터 정답\n",
    "\n",
    "    n_train = input_train.shape[0]  # 훈련데이터 샘플 수\n",
    "    n_test = input_test.shape[0]  # 테스트데이터 샘플 수\n",
    "\n",
    "# -- 각 설정 값 --\n",
    "    n_in = 4  # 입력층 뉴런 수\n",
    "    n_mid = 25  # 은닉층 뉴런 수\n",
    "    n_out = 3  # 출력층 뉴런 수\n",
    "\n",
    "    wb_width = 0.1  # 가중치와 편향 설정을 위한 정규분포 표준편차\n",
    "    eta = 0.01  # 학습률\n",
    "    epoch = 1000\n",
    "    batch_size = 8\n",
    "    interval = 100  # 경과 표시 간격\n",
    "    n_batch = n_train // batch_size  # 1에포크 당 배치 수\n",
    "   \n",
    "    #2) 클래스 구현\n",
    "    class BaseLayer:\n",
    "        def __init__(self,n_upper,n): #상위층에 뉴런의 갯수, 현재 층의 뉴런의 갯수\n",
    "            self.w=wb_width*np.random.randn(n_upper,n) # 가중치와 편향 랜덤 주기\n",
    "            self.b=wb_width*np.random.randn(n)\n",
    "            self.h_w=np.zeros((n_upper,n)) + 1e-8 #.h는 아다그라드를 위해\n",
    "            self.h_b=np.zeros((n)) + 1e-8\n",
    "        \n",
    "        def update(self,eta):\n",
    "            self.h_w+=self.grad_w * self.grad_w\n",
    "            self.h_b+=self.grad_b * self.grad_b\n",
    "        \n",
    "            self.w -=eta/np.sqrt(self.h_w)*self.grad_w # 가중치와 편향의 기울기 * 학습률\n",
    "            self.b -=eta/np.sqrt(self.h_b)*self.grad_b\n",
    "\n",
    "    class MiddleLayer(BaseLayer): #상속 은닉층\n",
    "        #순방향 \n",
    "        def forward(self,x):\n",
    "            self.x=x\n",
    "            self.u=np.dot(x,self.w) + self.b # 행렬 곱 가중치와 입력들어온 x값\n",
    "            self.y=np.where(self.u<=0,0,self.u) # (활성화함수)출력 하는거임 ReLu 함수 0보다 작으면 0, 아니면 그 자신\n",
    "        \n",
    "        #역방향\n",
    "        def backward(self,grad_y):\n",
    "            delta=grad_y*np.where(self.u<=0,0,1) # 출력기울기*relu 함수 미분\n",
    "            self.grad_w=np.dot(self.x.T,delta)#가중치와 편향와 입력의 기울기를 구해야함.\n",
    "            self.grad_b=np.sum(delta,axis=0)\n",
    "            self.grad_x=np.dot(delta,self.w.T) #T는 전치행렬\n",
    "\n",
    "    class OutputLayer(BaseLayer): #상속 출력층\n",
    "        #순방향 \n",
    "        def forward(self,x):\n",
    "            self.x=x\n",
    "            self.u=np.dot(x,self.w) + self.b\n",
    "            self.y=np.exp(self.u)/np.sum(np.exp(self.u),axis=1,keepdims=True)\n",
    "        \n",
    "        #역방향\n",
    "        def backward(self,t): # t는 정답 정답을 입력으로 받음\n",
    "            delta=self.y-t\n",
    "            self.grad_w=np.dot(self.x.T,delta)#가중치와 편향와 입력의 기울기를 구해야함.\n",
    "            self.grad_b=np.sum(delta,axis=0)\n",
    "            self.grad_x=np.dot(delta,self.w.T) #T는 전치행렬\n",
    "        \n",
    "    #3)신경망 구현\n",
    "    #학습 알고리즘 - 미니배치 사용 \n",
    "    \n",
    "    middle_layer_1=MiddleLayer(n_in,n_mid)\n",
    "    middle_layer_2=MiddleLayer(n_mid,n_mid)\n",
    "    output_layer=OutputLayer(n_mid,n_out)\n",
    "\n",
    "    #순전파\n",
    "    def forward_propagation(x):\n",
    "        middle_layer_1.forward(x)\n",
    "        middle_layer_2.forward(middle_layer_1.y) #앞 뉴런의 출력\n",
    "        output_layer.forward(middle_layer_2.y) #위와 동일\n",
    "    \n",
    "    #역전파\n",
    "    def back_propagation(t):\n",
    "        output_layer.backward(t)\n",
    "        middle_layer_2.backward(output_layer.grad_x) #입력 기울기\n",
    "        middle_layer_1.backward(middle_layer_2.grad_x)\n",
    "    \n",
    "    #보정\n",
    "    def update_wb():\n",
    "        middle_layer_1.update(eta)\n",
    "        middle_layer_2.update(eta)\n",
    "        output_layer.update(eta)\n",
    "    \n",
    "    #교차엔트로피 에러 계산하는 함수\n",
    "    def get_error(t,batch_size): #8개를 다 구한다음 배치사이즈 만큼 나눠줌\n",
    "        return -np.sum(t*np.log(output_layer.y+1e-7))\n",
    "    \n",
    "    \n",
    "    # 학습 for문 두개로 / 후 출력\n",
    "\n",
    "    train_error_x=[] # 데이터의 횟수(갯수)\n",
    "    train_error_y=[] # 데이터의 오차값\n",
    "    test_error_x=[]\n",
    "    test_error_y=[]\n",
    "\n",
    "    for i in range(epoch):\n",
    "        forward_propagation(input_train)\n",
    "        error_train=get_error(correct_train,n_train) # 한번에 다 보겠다. 그래서 n_train 75개\n",
    "        forward_propagation(input_test)\n",
    "        error_test=get_error(correct_test,n_test)\n",
    "    \n",
    "        train_error_x.append(i)\n",
    "        train_error_y.append(error_train)\n",
    "        test_error_x.append(i)\n",
    "        test_error_y.append(error_test)\n",
    "    \n",
    "        index_random=np.arange(n_train)\n",
    "        np.random.shuffle(index_random)\n",
    "        for j in range(n_batch):\n",
    "            mb_index=index_random[j*batch_size:(j+1)*batch_size] # 0 부터 8 까지 0~7 j 가 1이면 9~15\n",
    "            x= input_train[mb_index,:]\n",
    "            t= correct_train[mb_index,:] #목표출력(정답)\n",
    "        \n",
    "            forward_propagation(x)\n",
    "            back_propagation(t)\n",
    "            update_wb()\n",
    "\n",
    "    plt.plot(train_error_x,train_error_y,label=\"Train\")\n",
    "    plt.plot(test_error_x,test_error_y,label=\"Test\")\n",
    "    plt.legend() # 각 그래프 이름\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "if overfitting=='2':\n",
    "    iris_data = datasets.load_iris()\n",
    "    input_data = iris_data.data\n",
    "    correct = iris_data.target\n",
    "    n_data = len(correct)  # 샘플 수\n",
    "\n",
    "# -- 입력 데이터 표준화 --\n",
    "    ave_input = np.average(input_data, axis=0)\n",
    "    std_input = np.std(input_data, axis=0)\n",
    "    input_data = (input_data - ave_input) / std_input\n",
    "\n",
    "# -- 정답을 원-핫 인코딩으로 변경 --\n",
    "    correct_data = np.zeros((n_data, 3))\n",
    "    for i in range(n_data):\n",
    "        correct_data[i, correct[i]] = 1.0\n",
    "\n",
    "# -- 훈련 데이터와 테스트 데이터 --\n",
    "    index = np.arange(n_data)\n",
    "    index_train = index[index%2 == 0]\n",
    "    index_test = index[index%2 != 0]\n",
    "\n",
    "    input_train = input_data[index_train, :]  # 훈련데이터 입력\n",
    "    correct_train = correct_data[index_train, :]  # 훈련데이터 정답\n",
    "    input_test = input_data[index_test, :]  # 테스트데이터 입력\n",
    "    correct_test = correct_data[index_test, :]  # 테스트데이터 정답\n",
    "\n",
    "    n_train = input_train.shape[0]  # 훈련데이터 샘플 수\n",
    "    n_test = input_test.shape[0]  # 테스트데이터 샘플 수\n",
    "\n",
    "# -- 각 설정 값 --\n",
    "    n_in = 4  # 입력층 뉴런 수\n",
    "    n_mid = 25  # 은닉층 뉴런 수\n",
    "    n_out = 3  # 출력층 뉴런 수\n",
    "\n",
    "    wb_width = 0.1  # 가중치와 편향 설정을 위한 정규분포 표준편차\n",
    "    eta = 0.01  # 학습률\n",
    "    epoch = 1000\n",
    "    batch_size = 8\n",
    "    interval = 100  # 경과 표시 간격\n",
    "    n_batch = n_train // batch_size  # 1에포크 당 배치 수\n",
    "   \n",
    "    #2) 클래스 구현\n",
    "    class BaseLayer:\n",
    "        def __init__(self,n_upper,n): #상위층에 뉴런의 갯수, 현재 층의 뉴런의 갯수\n",
    "            self.w=wb_width*np.random.randn(n_upper,n) # 가중치와 편향 랜덤 주기\n",
    "            self.b=wb_width*np.random.randn(n)\n",
    "        \n",
    "        def update(self,eta):\n",
    "            self.w -=eta*self.grad_w # 가중치와 편향의 기울기 * 학습률\n",
    "            self.b -=eta*self.grad_b\n",
    "\n",
    "    class MiddleLayer(BaseLayer): #상속 은닉층\n",
    "        #순방향 \n",
    "        def forward(self,x):\n",
    "            self.x=x\n",
    "            self.u=np.dot(x,self.w) + self.b # 행렬 곱 가중치와 입력들어온 x값\n",
    "            self.y=np.where(self.u<=0,0,self.u) # (활성화함수)출력 하는거임 ReLu 함수 0보다 작으면 0, 아니면 그 자신\n",
    "        \n",
    "        #역방향\n",
    "        def backward(self,grad_y):\n",
    "            delta=grad_y*np.where(self.u<=0,0,1) # 출력기울기*relu 함수 미분\n",
    "            self.grad_w=np.dot(self.x.T,delta)#가중치와 편향와 입력의 기울기를 구해야함.\n",
    "            self.grad_b=np.sum(delta,axis=0)\n",
    "            self.grad_x=np.dot(delta,self.w.T) #T는 전치행렬\n",
    "            \n",
    "    class Dropout(): #드롭 아웃층\n",
    "        def __init__(self,dropout_ratio): #ratio는 얼마만큼 비율로 없애줄거냐?\n",
    "            self.dropout_ratio=dropout_ratio\n",
    "        \n",
    "        #순방향 \n",
    "        def forward(self,x,is_train):\n",
    "            if is_train:\n",
    "                rand=np.random.rand(*x.shape) #은닉층 뉴런의 갯수만큼 생김.\n",
    "                self.dropout=np.where(rand>self.dropout_ratio,1,0)\n",
    "                self.y=x*self.dropout\n",
    "            else:\n",
    "                self.y=x*(1-self.dropout_ratio)\n",
    "        \n",
    "    #역방향\n",
    "        def backward(self,grad_y):\n",
    "           self.grad_x=grad_y*self.dropout #드롭아웃이 1이면 전달 0이면 전달  X\n",
    "\n",
    "    class OutputLayer(BaseLayer): #상속 출력층\n",
    "        #순방향 \n",
    "        def forward(self,x):\n",
    "            self.x=x\n",
    "            self.u=np.dot(x,self.w) + self.b\n",
    "            self.y=np.exp(self.u)/np.sum(np.exp(self.u),axis=1,keepdims=True)\n",
    "        \n",
    "        #역방향\n",
    "        def backward(self,t): # t는 정답 정답을 입력으로 받음\n",
    "            delta=self.y-t\n",
    "            self.grad_w=np.dot(self.x.T,delta)#가중치와 편향와 입력의 기울기를 구해야함.\n",
    "            self.grad_b=np.sum(delta,axis=0)\n",
    "            self.grad_x=np.dot(delta,self.w.T) #T는 전치행렬\n",
    "        \n",
    "    #3)신경망 구현\n",
    "    #학습 알고리즘 - 미니배치 사용 \n",
    "    \n",
    "    middle_layer_1=MiddleLayer(n_in,n_mid)\n",
    "    dropout1=Dropout(0.5)\n",
    "    middle_layer_2=MiddleLayer(n_mid,n_mid)\n",
    "    dropout2=Dropout(0.5)\n",
    "    output_layer=OutputLayer(n_mid,n_out)\n",
    "\n",
    "    #순전파\n",
    "    def forward_propagation(x,is_train):\n",
    "        middle_layer_1.forward(x)\n",
    "        dropout1.forward(middle_layer_1.y,is_train)\n",
    "        middle_layer_2.forward(middle_layer_1.y) #앞 뉴런의 출력\n",
    "        dropout2.forward(middle_layer_2.y,is_train)\n",
    "        output_layer.forward(middle_layer_2.y) #위와 동일\n",
    "    \n",
    "    #역전파\n",
    "    def back_propagation(t):\n",
    "        output_layer.backward(t)\n",
    "        dropout2.backward(output_layer.grad_x)\n",
    "        middle_layer_2.backward(dropout2.grad_x) #입력 기울기\n",
    "        dropout1.backward(middle_layer_2.grad_x)\n",
    "        middle_layer_1.backward(dropout1.grad_x)\n",
    "    \n",
    "    #보정\n",
    "    def update_wb():\n",
    "        middle_layer_1.update(eta)\n",
    "        middle_layer_2.update(eta)\n",
    "        output_layer.update(eta)\n",
    "    \n",
    "    #교차엔트로피 에러 계산하는 함수\n",
    "    def get_error(t,batch_size): #8개를 다 구한다음 배치사이즈 만큼 나눠줌\n",
    "        return -np.sum(t*np.log(output_layer.y+1e-7))\n",
    "    \n",
    "    \n",
    "    # 학습 for문 두개로 / 후 출력\n",
    "\n",
    "    train_error_x=[] # 데이터의 횟수(갯수)\n",
    "    train_error_y=[] # 데이터의 오차값\n",
    "    test_error_x=[]\n",
    "    test_error_y=[]\n",
    "\n",
    "    for i in range(epoch):\n",
    "        forward_propagation(input_train,False)\n",
    "        error_train=get_error(correct_train,n_train) # 한번에 다 보겠다. 그래서 n_train 75개\n",
    "        forward_propagation(input_test,False)\n",
    "        error_test=get_error(correct_test,n_test)\n",
    "    \n",
    "        train_error_x.append(i)\n",
    "        train_error_y.append(error_train)\n",
    "        test_error_x.append(i)\n",
    "        test_error_y.append(error_test)\n",
    "    \n",
    "        index_random=np.arange(n_train)\n",
    "        np.random.shuffle(index_random)\n",
    "        for j in range(n_batch):\n",
    "            mb_index=index_random[j*batch_size:(j+1)*batch_size] # 0 부터 8 까지 0~7 j 가 1이면 9~15\n",
    "            x= input_train[mb_index,:]\n",
    "            t= correct_train[mb_index,:] #목표출력(정답)\n",
    "        \n",
    "            forward_propagation(x,True)\n",
    "            back_propagation(t)\n",
    "            update_wb()\n",
    "\n",
    "    plt.plot(train_error_x,train_error_y,label=\"Train\")\n",
    "    plt.plot(test_error_x,test_error_y,label=\"Test\")\n",
    "    plt.legend() # 각 그래프 이름\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.show()\n",
    "    \n",
    "if overfitting=='3':\n",
    "    iris_data = datasets.load_iris()\n",
    "    input_data = iris_data.data\n",
    "    correct = iris_data.target\n",
    "    n_data = len(correct)  # 샘플 수\n",
    "\n",
    "# -- 입력 데이터 표준화 --\n",
    "    ave_input = np.average(input_data, axis=0)\n",
    "    std_input = np.std(input_data, axis=0)\n",
    "    input_data = (input_data - ave_input) / std_input\n",
    "\n",
    "# -- 정답을 원-핫 인코딩으로 변경 --\n",
    "    correct_data = np.zeros((n_data, 3))\n",
    "    for i in range(n_data):\n",
    "        correct_data[i, correct[i]] = 1.0\n",
    "\n",
    "# -- 훈련 데이터와 테스트 데이터 --\n",
    "    index = np.arange(n_data)\n",
    "    index_train = index[index%2 == 0]\n",
    "    index_test = index[index%2 != 0]\n",
    "\n",
    "    input_train = input_data[index_train, :]  # 훈련데이터 입력\n",
    "    correct_train = correct_data[index_train, :]  # 훈련데이터 정답\n",
    "    input_test = input_data[index_test, :]  # 테스트데이터 입력\n",
    "    correct_test = correct_data[index_test, :]  # 테스트데이터 정답\n",
    "\n",
    "    n_train = input_train.shape[0]  # 훈련데이터 샘플 수\n",
    "    n_test = input_test.shape[0]  # 테스트데이터 샘플 수\n",
    "\n",
    "# -- 각 설정 값 --\n",
    "    n_in = 4  # 입력층 뉴런 수\n",
    "    n_mid = 25  # 은닉층 뉴런 수\n",
    "    n_out = 3  # 출력층 뉴런 수\n",
    "\n",
    "    wb_width = 0.1  # 가중치와 편향 설정을 위한 정규분포 표준편차\n",
    "    eta = 0.01  # 학습률\n",
    "    epoch = 1000\n",
    "    batch_size = 8\n",
    "    interval = 100  # 경과 표시 간격\n",
    "    n_batch = n_train // batch_size  # 1에포크 당 배치 수\n",
    "   \n",
    "    #2) 클래스 구현\n",
    "    class BaseLayer:\n",
    "        def __init__(self,n_upper,n): #상위층에 뉴런의 갯수, 현재 층의 뉴런의 갯수\n",
    "            self.w=wb_width*np.random.randn(n_upper,n) # 가중치와 편향 랜덤 주기\n",
    "            self.b=wb_width*np.random.randn(n)\n",
    "            self.h_w=np.zeros((n_upper,n)) + 1e-8 #.h는 아다그라드를 위해\n",
    "            self.h_b=np.zeros((n)) + 1e-8\n",
    "        \n",
    "        def update(self,eta):\n",
    "            self.h_w+=self.grad_w * self.grad_w\n",
    "            self.h_b+=self.grad_b * self.grad_b\n",
    "        \n",
    "            self.w -=eta/np.sqrt(self.h_w)*self.grad_w # 가중치와 편향의 기울기 * 학습률\n",
    "            self.b -=eta/np.sqrt(self.h_b)*self.grad_b\n",
    "\n",
    "    class MiddleLayer(BaseLayer): #상속 은닉층\n",
    "        #순방향 \n",
    "        def forward(self,x):\n",
    "            self.x=x\n",
    "            self.u=np.dot(x,self.w) + self.b # 행렬 곱 가중치와 입력들어온 x값\n",
    "            self.y=np.where(self.u<=0,0,self.u) # (활성화함수)출력 하는거임 ReLu 함수 0보다 작으면 0, 아니면 그 자신\n",
    "        \n",
    "        #역방향\n",
    "        def backward(self,grad_y):\n",
    "            delta=grad_y*np.where(self.u<=0,0,1) # 출력기울기*relu 함수 미분\n",
    "            self.grad_w=np.dot(self.x.T,delta)#가중치와 편향와 입력의 기울기를 구해야함.\n",
    "            self.grad_b=np.sum(delta,axis=0)\n",
    "            self.grad_x=np.dot(delta,self.w.T) #T는 전치행렬\n",
    "            \n",
    "    class Dropout(): #드롭 아웃층\n",
    "        def __init__(self,dropout_ratio): #ratio는 얼마만큼 비율로 없애줄거냐?\n",
    "            self.dropout_ratio=dropout_ratio\n",
    "        \n",
    "        #순방향 \n",
    "        def forward(self,x,is_train):\n",
    "            if is_train:\n",
    "                rand=np.random.rand(*x.shape) #은닉층 뉴런의 갯수만큼 생김.\n",
    "                self.dropout=np.where(rand>self.dropout_ratio,1,0)\n",
    "                self.y=x*self.dropout\n",
    "            else:\n",
    "                self.y=x*(1-self.dropout_ratio)\n",
    "        \n",
    "    #역방향\n",
    "        def backward(self,grad_y):\n",
    "           self.grad_x=grad_y*self.dropout #드롭아웃이 1이면 전달 0이면 전달  X\n",
    "\n",
    "    class OutputLayer(BaseLayer): #상속 출력층\n",
    "        #순방향 \n",
    "        def forward(self,x):\n",
    "            self.x=x\n",
    "            self.u=np.dot(x,self.w) + self.b\n",
    "            self.y=np.exp(self.u)/np.sum(np.exp(self.u),axis=1,keepdims=True)\n",
    "        \n",
    "        #역방향\n",
    "        def backward(self,t): # t는 정답 정답을 입력으로 받음\n",
    "            delta=self.y-t\n",
    "            self.grad_w=np.dot(self.x.T,delta)#가중치와 편향와 입력의 기울기를 구해야함.\n",
    "            self.grad_b=np.sum(delta,axis=0)\n",
    "            self.grad_x=np.dot(delta,self.w.T) #T는 전치행렬\n",
    "        \n",
    "    #3)신경망 구현\n",
    "    #학습 알고리즘 - 미니배치 사용 \n",
    "    \n",
    "    middle_layer_1=MiddleLayer(n_in,n_mid)\n",
    "    dropout1=Dropout(0.5)\n",
    "    middle_layer_2=MiddleLayer(n_mid,n_mid)\n",
    "    dropout2=Dropout(0.5)\n",
    "    output_layer=OutputLayer(n_mid,n_out)\n",
    "\n",
    "    #순전파\n",
    "    def forward_propagation(x,is_train):\n",
    "        middle_layer_1.forward(x)\n",
    "        dropout1.forward(middle_layer_1.y,is_train)\n",
    "        middle_layer_2.forward(middle_layer_1.y) #앞 뉴런의 출력\n",
    "        dropout2.forward(middle_layer_2.y,is_train)\n",
    "        output_layer.forward(middle_layer_2.y) #위와 동일\n",
    "    \n",
    "    #역전파\n",
    "    def back_propagation(t):\n",
    "        output_layer.backward(t)\n",
    "        dropout2.backward(output_layer.grad_x)\n",
    "        middle_layer_2.backward(dropout2.grad_x) #입력 기울기\n",
    "        dropout1.backward(middle_layer_2.grad_x)\n",
    "        middle_layer_1.backward(dropout1.grad_x)\n",
    "    \n",
    "    #보정\n",
    "    def update_wb():\n",
    "        middle_layer_1.update(eta)\n",
    "        middle_layer_2.update(eta)\n",
    "        output_layer.update(eta)\n",
    "    \n",
    "    #교차엔트로피 에러 계산하는 함수\n",
    "    def get_error(t,batch_size): #8개를 다 구한다음 배치사이즈 만큼 나눠줌\n",
    "        return -np.sum(t*np.log(output_layer.y+1e-7))\n",
    "    \n",
    "    \n",
    "    # 학습 for문 두개로 / 후 출력\n",
    "\n",
    "    train_error_x=[] # 데이터의 횟수(갯수)\n",
    "    train_error_y=[] # 데이터의 오차값\n",
    "    test_error_x=[]\n",
    "    test_error_y=[]\n",
    "\n",
    "    for i in range(epoch):\n",
    "        forward_propagation(input_train,False)\n",
    "        error_train=get_error(correct_train,n_train) # 한번에 다 보겠다. 그래서 n_train 75개\n",
    "        forward_propagation(input_test,False)\n",
    "        error_test=get_error(correct_test,n_test)\n",
    "    \n",
    "        train_error_x.append(i)\n",
    "        train_error_y.append(error_train)\n",
    "        test_error_x.append(i)\n",
    "        test_error_y.append(error_test)\n",
    "    \n",
    "        index_random=np.arange(n_train)\n",
    "        np.random.shuffle(index_random)\n",
    "        for j in range(n_batch):\n",
    "            mb_index=index_random[j*batch_size:(j+1)*batch_size] # 0 부터 8 까지 0~7 j 가 1이면 9~15\n",
    "            x= input_train[mb_index,:]\n",
    "            t= correct_train[mb_index,:] #목표출력(정답)\n",
    "        \n",
    "            forward_propagation(x,True)\n",
    "            back_propagation(t)\n",
    "            update_wb()\n",
    "\n",
    "    plt.plot(train_error_x,train_error_y,label=\"Train\")\n",
    "    plt.plot(test_error_x,test_error_y,label=\"Test\")\n",
    "    plt.legend() # 각 그래프 이름\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
